/*
TITLE: ALGORITHMS             SUBTITLE:P1
AUTHOR 1:PAULA TAIBO SUAREZ   LOGIN:P.TAIBO
AUTHOR 2:SIYUAN HE            LOGIN:SIYUAN.HE
GROUP:6.1                     DATE:03/02/2023
*/

This practical was execute in a laptop with the next configuration:

Machine:
OS:
Kernel:
RAM:  
Batery:


The purpose of the practical is to obtain the biggest consecutive sum from a given
array of integers. To achive this, two different algorithms were employed and then compared.

For the first test we run the algorithms using a predifined set of arrays 
to confirm they worked correctly.

TEST 1:
                         maxSubSum1     maxSubSum2
 -9  2 -5 -4  6              6              6
  4  0  9  2  5             20             20
 -2 -1 -9 -7 -1              0              0
  9 -2  1 -7 -8              9              9
 15 -2 -5 -4 16             20             20
  7 -5  6  7 -7             15             15

In the second test we generated a random set of arrays and 
asertained that the algorithms worked correctly by checking 
if they arrived at the same results.

TEST 2:
                                      maxSubSum1     maxSubSum2
[ -2 -6 -1 -9 -1 -6  3  9  5]             17             17
[  5 -9 -3 -2  2  9 -2 -8 -3]             11             11
[  6 -3  9 -4  5  6 -3  0  3]             19             19
[ -7  5 -2  3 -9  1  8 -9  6]              9              9
[ -8  3  5  6 -4  3 -7  0 -5]             14             14
[ -7  7 -7 -4  3 -1 -5  8 -9]              8              8
[  7 -7 -2 -2 -8 -7  5  4 -7]              9              9
[ -6 -1  9  9  0  0  2 -4  5]             21             21
[ -5 -4 -5 -1 -2  8 -1  0 -2]              8              8
[  4  5 -6  5 -1 -7 -7  6 -6]              9              9

Now that we have confirmed that both algorithms work, we can determine their 
execution times by testing them with random arrays of lenght 
500, 1000, 2000, 4000, 8000, 16000 and 32000.

SubMax1:
   n	  t(n)		   t(n)/n^1.8	t(n)/n^2	   t(n)/n^2.2
  500	875.000000	    0.042039	0.003500		0.000542		
 1000	3330.000000	    0.052777	0.003330		0.000419		
 2000	14635.000000	0.076515	0.003659		0.000374		
 4000	54902.000000	0.094687	0.003431		0.000285		
 8000	211649.000000	0.120413	0.003307		0.000223		
16000	829688.000000	0.155712	0.003241		0.000178		
32000	3301544.000000	0.204398	0.003224		0.000144		

Slightly underestimated bound: n^1.8.
Tight bound: n^2.
Slightly overestimated bound:n^2.2.

SubMax2:
   n	  t(n)	   t(n)/n^0.8	 t(n)/n		 t(n)/n^1.2
  500	2.771000	0.019207	0.005542	  0.001599		
 1000	5.339000	0.021255	0.005339	  0.001341		
 2000	10.472000	0.023944	0.005236	  0.001145		
 4000	21.274000	0.027938	0.005319	  0.001012		
 8000	45.457000	0.034287	0.005682	  0.000942		
16000	90.278000	0.039110	0.005642	  0.000814		
32000	168.802000	0.042001	0.005275	  0.000663

Slightly underestimated bound: n^0.8.
Tight bound: n.
Slightly overestimated bound:n^1.2.

The execution times were measured in microseconds. Analysing them we can observe 
that the first algorithm takes more than 500μs, thus we can be confident that 
the measurements are accurate. On the other hand, the second algorithm never 
surpasses the 500μs threshold, so the aproximate time has to be calculated by 
executing it K times and computing the mean. The constant K has to be a power of 
10 for the measurement to be correct so we chose 1000.

For the first algorithm we presumed cuadratic complexity. To empericaly demostrate our hipothesis we
made the calculations with our estimateted bound and a higher and a lower bounds. 
Analysing the results we observe the hypothesized tight bound tends to the the constant
0.003 while the other two tend to zero and infinity, respectively. This proves that the 
complexity of the algorithm is indeed cuadratic.

For the second algorithm we assumed linear complexity. In this case the constant tended 
to 0.005 in contrast with the repective lower and higher bounds which again tended to infinity
and zero, which is empirical proof that the estimated bound was correct.

Observations:

Some irregularities were noticed during the testing phase, such as unusual measurements that deviated 
from the average value. For example, in the second algorithm there is a couple of irregular measurements
that defer form the constant value.
	
SubMax2:
   n	  t(n)	   t(n)/n^0.8	 t(n)/n		   t(n)/n^1.2
  500	4.151000	0.028772 -> 0.008302 <-		0.002395		
 1000	6.207000	0.024711	0.006207 		0.001559		
 2000	11.625000	0.026581	0.005812		0.001271		
 4000	20.809000	0.027328	0.005202		0.000990		
 8000	44.600000	0.033641	0.005575		0.000924		
16000	89.356000	0.038710	0.005585		0.000806		
32000	172.731000	0.042978	0.005398		0.000678		

To ensure the accuracy of the results the tests were non-manually repeated enough times to garantee
those unusual measurements were anomalies.

Conclusion:

After running all these tests and empericaly analysing the results we have demostrated that 
the cuadratic and linear upper bounds are good approximations for the first and second algorithms, 
respectily. From this we can conclude that the second is more efficient than the first. 