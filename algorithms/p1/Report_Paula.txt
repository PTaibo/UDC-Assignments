/*
TITLE: ALGORITHMS             SUBTITLE:P1
AUTHOR 1:PAULA TAIBO SUAREZ   LOGIN:P.TAIBO
AUTHOR 2:SIYUAN HE            LOGIN:SIYUAN.HE
GROUP:6.1                     DATE:03/02/2023
*/

This practical was executed in a laptop with the next specifications:

OS: #1 SMP Debian 5.10.191-1 (2023-08-16)
Kernel: Linux 5.10.0-25-amd64
CPU: Four 2.8GHz Intel i5 Skylake Processors
RAM: 7.6GB RAM
Batery: 100% and plugged


The purpose of the practical is to obtain the biggest consecutive sum from
a given array of integers. To achieve this, two different algorithms were
employed and then compared.

For the first test we run the algorithms using a predifined set of arrays 
to confirm they worked correctly.

TEST 1:
                         maxSubSum1     maxSubSum2
 -9  2 -5 -4  6              6              6
  4  0  9  2  5             20             20
 -2 -1 -9 -7 -1              0              0
  9 -2  1 -7 -8              9              9
 15 -2 -5 -4 16             20             20
  7 -5  6  7 -7             15             15

In the second test we generated a random set of arrays and 
ascertained that the algorithms worked correctly by checking 
if they arrived at the same results.

TEST 2:
                                      maxSubSum1     maxSubSum2
[ -6  4  0 -2  9 -6 -9  9 -2]             11             11
[  1 -3 -3 -5 -6 -2  7 -8 -7]              7              7
[  7  5 -4  0 -9 -6  1  3 -6]             12             12
[ -3  4  7  2  4  1  8  8  1]             35             35
[ -7  9  0 -3  0  4  3  2  7]             22             22
[  8 -1  5  7 -7 -3  9  2 -3]             20             20
[ -7  9 -9 -7 -7  1 -9  1 -4]              9              9
[ -2 -1 -9  8 -2  9 -4  1  0]             15             15
[  9 -5  8  6  9 -6 -1  6 -4]             27             27
[  5  5  4 -7  4  3  9  3  2]             28             28

Now that we have confirmed that both algorithms work, we can determine their 
execution times by testing them with random arrays of the following lengths: 500, 1000, 2000,
4000, 8000, 16000 and 32000.

SubMax1:
   n	  t(n)		   t(n)/n^1.8	t(n)/n^2   t(n)/n^2.2
  500	362.659000	    0.017424	0.001451	0.000225		
 1000	1435.000000	    0.022743	0.001435	0.000181		
 2000	5695.000000	    0.029775	0.001424	0.000146		
 4000	24224.000000	0.041778	0.001514	0.000126		
 8000	94105.000000	0.053539	0.001470	0.000099		
16000	367744.000000	0.069017	0.001437	0.000079		
32000	1462430.000000	0.090539	0.001428	0.000064

Slightly underestimated bound: n^1.8.
Tight bound: n^2.
Slightly overestimated bound:n^2.2.

SubMax2:
   n	  t(n)	   t(n)/n^0.8	t(n)/n     t(n)/n^1.2
  500	1.472000	0.010203	0.002944	0.000849		
 1000	2.803000	0.011159	0.002803	0.000704		
 2000	5.960000	0.013628	0.002980	0.000652		
 4000	12.208000	0.016032	0.003052	0.000581		
 8000	24.005000	0.018106	0.003001	0.000497		
16000	48.108000	0.020841	0.003007	0.000434		
32000	96.350000	0.023974	0.003011	0.000378

Slightly underestimated bound: n^0.8.
Tight bound: n.
Slightly overestimated bound:n^1.2.

The execution times were measured in microseconds. Analysing them we can
observe that the first algorithm takes more that 500μs for all measurements
with n bigger than 500, thus we can be confident that they are correct. On the
other hand, the first test (n=500) and all the tests for the second algorithm 
never surpass the 500μs threshold, so the approximate time has to be calculated
by executing it K times and computing the mean. The constant K has to be a 
power of 10 for the measurement to be correct, so we chose 1000.

For the first algorithm we presumed cuadratic complexity. To emperically
demostrate our hipothesis we made the calculations with our estimated
bound and a higher and lower bounds. Analysing the results we observe
the hypothesized tight bound tends to the the constant 0.001 while the
other two tend to zero and infinity, respectively. This proves that the 
complexity of the algorithm is indeed cuadratic.

For the second algorithm we assumed linear complexity. In this case the
constant tended to 0.003 in contrast with the repective lower and higher
bounds which again tended to infinity and zero, which is empirical proof
that the estimated bound was correct.

Observations:

To ensure the accuracy of the results the tests were non-manually repeated.
From this we observed that, in each execution of the program, the first set
of tests, which corresponded to the cuadratic algorithm as it was the first
one to be run, presented some anomalous values instead of the expected 0.001
constant. This happened for two or three measurements before going back to the
predicted behaviour.

SubMax1:
   n	  t(n)		   t(n)/n^1.8	t(n)/n^2     t(n)/n^2.2
  500	2101.000000	    0.100942	0.008404 <-   0.001303
 1000	9535.000000	    0.151120	0.009535 <-   0.001200
 2000	22499.000000	0.117629	0.005625 <-   0.000575
 4000	27617.000000	0.047630	0.001726      0.000143
 8000	99204.000000	0.056440	0.001550      0.000105
16000	368760.000000	0.069207	0.001440      0.000079
32000	1477894.000000	0.091496	0.001443      0.000064

The reason is probably that it coincides with the start of the execution.
This is why we applied a non-manual repetition, because if it were repeated
manually, the problem would occur every time and we would not be able to
extract an accurate conclusion.

Conclusion:

After running all these tests and empirically analysing the results, we have
demonstrated that the cuadratic and linear upper bounds are good approximations
for the first and second algorithms, respectively. From this we can conclude that
the latter is more efficient than the former.

